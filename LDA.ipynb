{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import string\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "\n",
    "Note: This requires `wget` and `unzip` to be installed on your system.\n",
    "\n",
    "It retrieves data from http://www.cs.cornell.edu/people/pabo/movie-review-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subprocess.call(\"./get_data.sh\", shell=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "\n",
    "This reads in each speech as a list of words. In particular, it removes all punctuation, stopwords, lower-cases all the words, and splits on white space. Please be aware that you must have `nltk` installed on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    subdirs = os.listdir('scale_whole_review')\n",
    "    \n",
    "    docs = []\n",
    "    for n in subdirs:\n",
    "        new_base = 'scale_whole_review/' + n + '/txt.parag/'\n",
    "        files = os.listdir(new_base)\n",
    "        for file in files:\n",
    "            with open(new_base + file, 'rb') as f:\n",
    "                docs.append(str(f.read()))\n",
    "            \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    docs = [doc.translate(translator).lower().split() for doc in docs]\n",
    "    docs = [[w for w in doc if w not in stops] for doc in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the collapsed gibbs sampler\n",
    "\n",
    "See slides [here](https://n-s-f.github.io/talks/lda.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gibbs(docs, num_topics, iterations=5000):\n",
    "    num_docs = len(docs)\n",
    "    \n",
    "    corpus = list(set(chain(*docs)))\n",
    "    num_words = len(corpus)\n",
    "    corpus_lookup = dict(zip(corpus, range(num_words)))\n",
    "    \n",
    "    alpha = 1 / num_topics  # flat prior\n",
    "    beta = 1 / num_words  # flat prior\n",
    "    \n",
    "    \n",
    "    # Assuming flat priors\n",
    "    # Randomly initialize topic allocations\n",
    "    current_topics = [np.random.randint(0, num_topics, len(words))\n",
    "                      for words in docs]\n",
    "     \n",
    "    # Update counts\n",
    "    topic_counts = np.zeros(num_topics)\n",
    "    for doc_topics in current_topics:\n",
    "        for topic in doc_topics:\n",
    "            topic_counts[topic] += 1\n",
    "\n",
    "    document_counts = np.zeros((num_docs, num_topics))    \n",
    "    for d, topics in enumerate(current_topics):\n",
    "        for t in range(num_topics):\n",
    "            document_counts[d][t] = sum(topics == t)\n",
    "            \n",
    "    word_counts = np.zeros((num_topics, num_words))\n",
    "    for d, doc in enumerate(docs):\n",
    "        for w, word in enumerate(doc):\n",
    "            topic = current_topics[d][w]\n",
    "            word_id = corpus_lookup[word]\n",
    "            word_counts[topic][word_id] += 1\n",
    "            \n",
    "    # Begin the Gibbs sampler proper\n",
    "    for it in range(iterations):\n",
    "        if it % 10 == 0:\n",
    "            print('iteration:', it)\n",
    "        \n",
    "        for d, doc in enumerate(docs):\n",
    "            for w, word in enumerate(doc):\n",
    "                topic = current_topics[d][w]\n",
    "                document_counts[d][topic] -= 1\n",
    "                word_counts[topic][corpus_lookup[word]] -= 1\n",
    "                topic_counts[topic] -= 1\n",
    "\n",
    "                probs = np.zeros(num_topics)\n",
    "                \n",
    "                # Quadruple nested for loop! \n",
    "                for j in range(num_topics):\n",
    "                    probs[j] = ((\n",
    "                    (document_counts[d][j] + alpha)\n",
    "                     * (word_counts[j][corpus_lookup[word]] + beta))\n",
    "                     / (topic_counts[j] + (beta * num_words)))\n",
    "\n",
    "                probs = probs / np.sum(probs)\n",
    "                new_topic = np.where(np.random.multinomial(1, probs))[0][0]\n",
    "                current_topics[d][w] = new_topic\n",
    "                document_counts[d][new_topic] += 1\n",
    "                word_counts[new_topic][corpus_lookup[word]] += 1\n",
    "                topic_counts[new_topic] += 1\n",
    "\n",
    "    return current_topics, document_counts, word_counts, topic_counts, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct topics\n",
    "\n",
    "Given how often each word appears in a topic, we can reconstruct the distributions of words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_topics(results):\n",
    "    topic_distributions = []\n",
    "    for topic in range(results[2].shape[0]):\n",
    "        percentages = results[2][topic] / np.sum(results[2][topic])\n",
    "        dist = sorted(list(zip(results[4], percentages)), key=lambda x: -x[1])\n",
    "        topic_distributions.append(dist)\n",
    "    return topic_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyze the data\n",
    "\n",
    "We'll assume there are 20 topics overall in the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    }
   ],
   "source": [
    "docs = read_data()\n",
    "results = gibbs(docs, num_topics=20, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_distributions = reconstruct_topics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[[a[0] for a in t[:10]] for t in topic_distributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
